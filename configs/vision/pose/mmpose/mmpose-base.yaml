const:
  num_gpus: "len({trainer.gpus})"
  samples_per_gpu: 2 # NOTE this is train batch size!!
  val_samples_per_gpu: 1 # NOTE this is val batch size!!

trainer:
  # evaluate every 3 epochs
  check_val_every_n_epoch: 3

training:
  metrics:
    total_loss:
      when: "trn"
      name: TorchMetric
      args:
        name: "MeanMetric"
      update:
        value: "loss"
    acc_pose:
      when: "trn"
      name: TorchMetric
      args:
        name: "MeanMetric"
      update:
        value: "acc_pose"
    loss/heatmap:
      when: "trn"
      name: TorchMetric
      args:
        name: "MeanMetric"
      update:
        value: "heatmap_loss"

callbacks:
  

  #ModelCheckpoint:
  #  name: LightningCallback
  #  args:
  #    name: ModelCheckpoint
  #    args:
  #      monitor: "epoch_val/mAP/map"
  #      mode: "max"
  #      save_last: True
  #      save_top_k: 1


  LearningRateMonitor:
    name: LightningCallback
    args:
      name: "LearningRateMonitor"
      args:
        logging_interval: "epoch"

dataloader:
  base_dataloader: # depends on hardware:)
    sampler: null
    batch_sampler: null
    num_workers: "2*{const.num_gpus}"
    pin_memory: True
    collate_fn:
      name: mmcv_datacontainer_collate
  trn:
    shuffle: True
    # Extend default_collate to add support for:type:`~mmcv.parallel.DataContainer`.
  val:
    shuffle: False
  test:
    shuffle: False
